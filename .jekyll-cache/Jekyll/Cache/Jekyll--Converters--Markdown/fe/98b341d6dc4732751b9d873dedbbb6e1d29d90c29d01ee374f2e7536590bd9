I"ù	<p>I recently got a membership to a site hosting a boatload of private label rights (PLR) materialÂ (<a href="http://www.idplr.com/go.php?offer=alexdglove&amp;pid=4" target="_blank">IDPLR.com</a>). 99% of PLR items are scams, garbage, or are outdated, but if you have the time or tools to dig through it you can find some gems. (Just so we're clear, I'm not running any scams, I was just looking for some content for another project). Anyway, I got access to the site that had thousands of zip files but obviously didn't want to download them one by one. Administrator wouldn't give me any way to download the files in bulk either, so I was left with no choice but to automate...</p>
<p>This is one of those projects that can be done a dozen different ways. There's probably a real slick way to do this with one line of code, but this method worked for me.</p>
<h2>Step 1 - Identify your targets</h2>
<p>This was the most challenging part for me, until I got lucky and found <a href="http://blog.adlibre.org/2011/06/03/extracting-all-links-website-using-wget/" target="_blank">a blog post</a> on how to grab EVERY link on a site using wget.Â I can't take any credit for this script, so I'll just share my slightly modified version:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">MYSITE</span><span class="o">=</span><span class="s1">'http://www.idplr.com'</span>
wget <span class="nt">-nv</span> <span class="nt">-r</span> <span class="nt">--spider</span> <span class="nv">$MYSITE</span> 2&gt;&amp;1 | egrep <span class="s1">' URL:'</span> | <span class="nb">awk</span> <span class="s1">'{print $3}'</span> | <span class="nb">sed</span> <span class="s2">"s@URL:</span><span class="k">${</span><span class="nv">MYSITE</span><span class="k">}</span><span class="s2">@@g"</span> <span class="o">&gt;&gt;</span> links.txt
</code></pre></div></div>
<p>This will create a file, links.txt, that contains EVERY link found on the site you specified for your "MYSITE" variable. Â Very powerful one-liner for sure. Keep in mind it may take a very long time for this command to complete (about an hour in my case).</p>
<p>We don't want to download every file on the site, so we need to whittle down our list. In my case, all of the files I wanted to download were conveniently in a directory labeled "downloads" (albeit in several different child directories) so I used the following command to create a new file (downloads.txt) that contained only the links to the "downloads" directory.</p>
:ET